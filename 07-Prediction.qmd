# Prediction {#prediction}

In this section, we move to our next social science goal

-   Describe
-   Explain, evaluate, and recommend $\rightarrow$ Causality
-   ***Predict***
-   Discover

Most of the tools we have been working on thus far have focused on first describing our data and then conducting tests through different types of comparisons and visualizations, in order to assess a deductive hypothesis, explaining the relationship between two variables.

Now we turn to a different goal.

Recall the difference between Correlation vs. Causality using our graphic showing the popularity of *Duck Dynasty* in different parts of the country. In 2016, researchers at the [NY Times](https://www.nytimes.com/interactive/2016/12/26/upshot/duck-dynasty-vs-modern-family-television-maps.html) noticed that areas in the country where the television show Duck Dynasty was popular also tended to support Donald Trump at higher rates.

![](images/duckdynasty.png){width="70%"}

For those used to working with the goal of explanation, shifting to prediction and classification may mean we need to shift what types of information we think is important.

-   Correlation: Areas that watch Duck Dynasty are more likely to support Trump (degree to which two variables \`\`move together")
-   Causality: Watching Duck Dynasty (vs. not watching) causes you to support Trump.

If we were interested in the goal of explaining voting decisions (what causes someone to vote a certain way?), we might not care if someone watches the show. However, if we were just interested in predicting vote share or voting decisions, a strong correlation could still be useful. Without spending a single dollar on surveying a community, we might have a general sense of their support for a candidate.

## Prediction Overview

Our goal: Predict (estimate/guess) some unknown using information we have as accurately and precisely as possible

-   Prediction could involve estimating a numeric outcome. Alternatively, prediction also involves classification-- predicting a categorical outcome (e.g., prediction of who wins vs. who loses).

Some political science examples of this might include

1.  Categorizing comments on social media as being toxic/nasty/uncivil

![Wired](images/instagramhate.png)

2.  Detecting Fake news and misinformation

![PBS](images/fakenewsai.png)

3.  Forecasting election results

![](images/forecast24.png)

Other examples

-   Trying to detect hate speech online
-   Predicting where or when an attack might occur
-   Trying to classify a large amount of text into subject or topic categories for analysis

What other types of things might we try to predict or classify in political science?

## Process of Prediction

Predict (estimate/guess) some unknown using information we have -- and do so as accurately and precisely as possible.

1.  Choose an approach
    -   Using an observed (known) measure as a direct proxy to predict an outcome
    -   Using one or more observed (known) measures in a regression model to predict an outcome
    -   (Beyond the course) Using a statistical model to select the measures to use for predicting an outcome
2.  Assess accuracy and precision
    -   Prediction error: $Prediction - Truth$
    -   Bias: Average prediction error: $\text{mean}(Prediction - Truth)$
        -   A prediction is \`unbiased' if the bias is zero (If the prediction is on average true)
    -   Root-mean squared error: $\sqrt{\text{mean}((Prediction - Truth)^2)}$
        -   Like \`absolute' error-- the average magnitude of the prediction error
        -   the typical distance the prediction is from the truth
    -   Confusion Matrix
        -   A cross-tab of predictions you got correct vs. predictions you got wrong (misclassified)
        -   Gives you true positives and true negatives vs. false positives and false negatives
3.  Iterate to improve the prediction/classification
    -   Often, we repeat steps 1-3 until we are confident in your method for predicting.
4.  Danger Zone: Eventually, after you have tested the approach and are satisfied with the accuracy, you may start applying it to new data for which you do not know the right answer.

## Example: Forecasting 2024 US Election based on 2020 Results

Let's try to predict the 2024 election results using just the 2020 results.

*For a video explainer of the code for a similar application, see below. (Via youtube, you can speed up the playback to 1.5 or 2x speed.)*

{{< video src="https://www.youtube.com/watch?v=zWDxZogRwOs" >}}

```{r, include=FALSE, echo=FALSE, warning=FALSE, message=FALSE, eval=FALSE}




library(tidyverse)
elecresults <- read.csv("https://raw.githubusercontent.com/fivethirtyeight/election-results/refs/heads/main/election_results_presidential.csv")
elecresults24 <- elecresults %>%
  filter(
    cycle == 2024,
    stage == "general",
    candidate_name %in% c("Donald Trump", "Kamala Harris"),
    ballot_party %in% c("DEM", "REP"),
    state != "Puerto Rico"
  ) %>%
  select(state_abbrev, state, candidate_name, percent) %>%
  pivot_wider(
    names_from = candidate_name,
    values_from = percent,
    names_sep = "_"
  ) 
elecresults24$dem_percent2024 <- elecresults24$`Kamala Harris`/100
elecresults24$rep_percent2024 <- elecresults24$`Donald Trump`/100
elecresults24 <- elecresults24 %>% select(-c(`Kamala Harris`, `Donald Trump`))
ev_table <- tribble(
    ~state_abbrev, ~ev_total,
    "AL", 9,  "AK", 3,  "AZ", 11, "AR", 6,  "CA", 54,
    "CO", 10, "CT", 7,  "DE", 3,  "DC", 3,  "FL", 30,
    "GA", 16, "HI", 4,  "ID", 4,  "IL", 19, "IN", 11,
    "IA", 6,  "KS", 6,  "KY", 8,  "LA", 8,  "ME", 2, "M1", 1, "M2", 1,
    "MD", 10, "MA", 11, "MI", 15, "MN", 10, "MS", 6,
    "MO", 10, "MT", 4,  "NE", 2, "N1", 1, "N2", 1, "N3", 1,  "NV", 6,  "NH", 4,
    "NJ", 14, "NM", 5,  "NY", 28, "NC", 16, "ND", 3,
    "OH", 17, "OK", 7,  "OR", 8,  "PA", 19, "RI", 4,
    "SC", 9,  "SD", 3,  "TN", 11, "TX", 40, "UT", 6,
    "VT", 3,  "VA", 13, "WA", 12, "WV", 4,  "WI", 10,
    "WY", 3
  ) %>%
  distinct(state_abbrev, .keep_all = TRUE) 

elecresults24 <- merge(elecresults24, ev_table, by="state_abbrev")


results2020$state_abbrev <- ifelse(results2020$stateid == "NE1", "N1",
                                   ifelse(results2020$stateid == "NE2", "N2",
                                          ifelse(results2020$stateid == "NE3", "N3",
                                                 ifelse(results2020$stateid == "ME2", "M2",
                                                        ifelse(results2020$stateid == "ME1", "M1",
                                                               as.character(results2020$stateid) )))))
results2020$called2016 <- ifelse(results2020$margin2016 <0, "R", "D")
results2020$called2020 <- results2020$called
results2020$dem_percent2020 <- results2020$dem_percent
results2020$rep_percent2020 <- results2020$rep_percent

results2020sub <- results2020 %>% select(state_abbrev, called2020,
                                         dem_percent2020,
                                         rep_percent2020,
                                         called2016)
table(results2020$called2016, results2020$called)
elecresults24 <- merge(elecresults24, results2020sub, by="state_abbrev")


elecresults24$called2024 <- ifelse(elecresults24$rep_percent2024 > elecresults24$dem_percent2024, "R","D")

write.csv(elecresults24, "data/elecresults24.csv")
```

```{r, include=F}
results2024 <- read.csv("data/elecresults24.csv", stringsAsFactors = T)
```

```{r, eval=F}
results2024 <- read.csv("elecresults24.csv", stringsAsFactors = T)
```

Variables

-   `state_abbrev` and `state`: state or state and district
-   `called2024`: result of 2024 election, R or D
-   `called2020`: result of 2020 election, R or D
-   `called2016`: result of 2016 election, R or D
-   `dem_percent24`: percentage for the Democratic candidate
-   `rep_percent24`: percentage for the Republican candidate
-   `dem_percent20`: percentage for the Democratic candidate
-   `rep_percent20`: percentage for the Republican candidate
-   `ev_total`: Electoral votes associated with a state/ district of a state during the 2024 election

![](images/results24map.png){width="80%"}

```{r}
sum(results2024$ev_total[results2024$called2024 == "R"])
sum(results2024$ev_total[results2024$called2024 == "D"])
```

### Choose Approach

1)  Choose an approach: Using an observed (known) measure as a direct proxy to predict an outcome

-   Let's use the 2020 result as a direct proxy to predict 2024.

```{r}
results2024$predicted2024 <- results2024$called2020
```

### Assess Accuracy

2)  Assess accuracy

What proportion of states did we get correct?

```{r}
mean(results2024$predicted2024 == results2024$called2024)
```

### Classification

We want to correctly predict the winner of each state

Prediction of binary outcome variable = classification problem

-   **true positive**: correctly predicting Trump to be the winner
-    **false positive **: incorrectly predicting Trump to be the winner (misclassification)
-    **true negative **: correctly predicting Trump to be the loser
-    **false negative **: incorrectly predicting Trump to be the loser (misclassification)

We define one outcome as the "positive" and one as the "negative." For now, we will say a Trump win is the positive and a Harris win is the negative.  This terminology comes from settings where a positive result just means "an event has occurred" (e.g., a positive medical test result might mean, yes, you broke your leg).  You could flip this and make a Harris win the positive and a Trump win the negative as long as you interpret things correctly.

Confusion Matrix: Tells us how we went right, how we went wrong.

```{r}
table(predicted=results2024$predicted2024, actual = results2024$called2024)
```


Which states did we get wrong?

```{r}
results2024$state[results2024$predicted2024 != results2024$called2024]
```

### Iterate to improve predictions

Start back at step one. We continue to repeat steps 1 and 2 until we are confident in our predictions.

How could we improve our predictions of elections? What other information could we use?

## Polling as Predictive


Many forecasters use pre-election polls in their models to predict election outcomes. In 2016 and 2020, polling-based forecasts received a lot of criticism

Prior to the 2016 elections, forecasts that used polls seemed confident that Hillary Clinton would win the election. [Political analysts](https://www.youtube.com/watch?v=zerWCVpXTr8) also seemed to think the polls were favorable to Clinton.

![*NY Upshot*](images/clintonupshot.png)



We all know that afterwards, Clinton did not win.

![*Pew*](images/afterpolls.png){width="70%"}


This led public opinion scholars and practitioners to do a deep investigation into the quality of pre-election polling. Like 2016, following the 2020 election, a similar team investigated the quality of pre-election polling in 2020. Here, while many polls pointed to a favorable outcome for Biden, the results seemed closer than one might have anticipated.

![](images/aaportitle.png)

The results of these findings are in the [AAPOR report](https://www.aapor.org/AAPOR_Main/media/MainSiteFiles/AAPOR-Task-Force-on-2020-Pre-Election-Polling_Report-FNL.pdf).


### Choose an Approach: Let's use some polls!

We will load polls from the 2024 election cycle. This cycle was unique because we had a change in candidates somewhat late in the campaign. Not all states were able to conduct polls with Harris on the ballot, and even if they did, not all polling aggregators have included data from these polls.

As a results, our polling dataset is somewhat incomplete. It contains polls only from states we might think are more competitive, or at least, more likely to be polled in an election.

In this next exercise, we will break our prediction activitiy into two sections. For states with polls available, we will make polling-based predictions for every state. For states without polls, we will rely on the 2016 party result as our prediction. We then combine both to get a prediction for the full election.

Let's load the data!

```{r, warning=F, message=F, include=FALSE, echo=FALSE, eval=FALSE}

#original 
polls24 <- read.csv("https://raw.githubusercontent.com/Lfirenzeg/msds607labs/main/president_polls.csv")

polls24sub <- polls24 %>% filter(office_type == "U.S. President")

polls24sub <- polls24sub %>%
  filter(
    candidate_name %in% c("Donald Trump", "Kamala Harris", "Joe Biden"),
    party %in% c("REP", "DEM"),
    election_date == "11/5/24",
    state != "Puerto Rico" & state != ""
  ) %>%
  select(state, poll_id, candidate_name, pct, pollster, start_date,
         election_date, question_id, sample_size, population) %>%
  pivot_wider(
    names_from = candidate_name,
    values_from = pct,
    names_sep = "_"
  ) 

polls24sub <- merge(polls24sub, elecresults24, by = "state")
polls24sub$start_date <- as.Date(polls24sub$start_date, format = "%m/%d/%y")
polls24sub$daystoelection <- as.Date("11/5/24",  format = "%m/%d/%y") - polls24sub$start_date
range(polls24sub$daystoelection, na.rm=T)
polls24sub$Harris <- polls24sub$`Kamala Harris`/100
polls24sub$Trump <- polls24sub$`Donald Trump`/100
polls24sub$samplesize <- polls24sub$sample_size 
polls24sub$sampletype <- polls24sub$population
polls24sub <- polls24sub %>% select(state, poll_id,Harris, Trump, pollster, daystoelection,election_date, question_id, samplesize,sampletype, colnames(elecresults24))

write.csv(polls24sub, file="polls24sub.csv")
```

```{r, include=F, eval=F}
polls24swing <- read.csv("data/silver24.csv")
polls24all <- read.csv("polls24sub.csv")
polls24allsub <- subset(polls24all, daystoelection < 200)
polls24allsub$X <- 1:nrow(polls24allsub)
polls24swing <- polls24swing %>% select(names(polls24swing)[names(polls24swing) %in% names(polls24allsub) ])
varnams <- names(polls24allsub)[names(polls24allsub) %in% names(polls24swing)]
polls24allsub <- polls24allsub %>% select(varnams)

polls24bothsources <- rbind(polls24swing, polls24allsub)
polls24bothsources <- subset(polls24bothsources, is.na(Harris) == F)
write.csv(polls24bothsources, "data/polls24bothsources.csv")

```

```{r, eval=F}
polls24 <- read.csv("polls24bothsources.csv")

```


```{r, include=F}
polls24 <- read.csv("data/polls24bothsources.csv")

```


Variables

-   `Harris`, `Trump`, poll-based percent going for each candidate
-   `state`: state or state and district
-   `samplesize`: sample size of the poll
-   `sampletype`: if it was a poll of likely voters, registered voters, or some other population
-   `daystoelection`: how close we are to 11/5/2024
-   `called2024`: result of 2024 election, R or D
-   `called2020`: result of 2020 election, R or D
-   `called2016`: result of 2016 election, R or D
-   `dem_percent24`: percentage for the Democratic candidate
-   `rep_percent24`: percentage for the Republican candidate
-   `dem_percent20`: percentage for the Democratic candidate
-   `rep_percent20`: percentage for the Republican candidate
-   `ev_total`: Electoral votes associated with a state/ district of a state during the 2024 election


Let's make a variable that shows us the predicted margin for Trump based on the poll in a given row, and a variable for the actual margin on Election Day.

```{r}
polls24$pollmargin24 <- polls24$Trump - polls24$Harris
polls24$resultmargin24 <- polls24$rep_percent2024 - polls24$dem_percent2024
```

Let's also see what states are in our polling data.

```{r}
## Iteration vector
states <- unique(polls24$state)
states
```


### Imagine the process for one state

For each state with polling data, we will make our prediction for the election by taking the average of the most recent polls in the state. We will define this as all polls within 15 days of the election, or, if some states only had polls taken much earlier, we will grab whatever the most recent poll's result was.


```{r}
## Subset to just Arizona
states[1]

subdata <- subset(polls24, state == states[1])

## Further subset to the "latest polls"
subdata <- subset(subdata, daystoelection < 15 | 
                      daystoelection == min(subdata$daystoelection) )
```

Now let's extract the actual margin for Trump, the poll-based predicted margin, and finally, let's assign electoral votes based on our prediction.

```{r}
## Find the margin for the actual result
result.marginAZ <- mean(subdata$resultmargin24)
result.marginAZ
## Find the margin for our prediction
polls.marginAZ <- mean(subdata$pollmargin24)
polls.marginAZ
## Allocate votes for Biden according to the margin
trumpvotesAZ <- ifelse(mean(subdata$pollmargin24) > 0, 
                            unique(subdata$ev_total), 0)
trumpvotesAZ
```

We predicted Trump would win Arizona  because the `polls.marginAZ` is positive. Therefore, we assigned Trump 11 electoral votes in this example.

### Loop through all states

Now let's repeat this process for all states. 

```{r}
## Iteration vector
states <- unique(polls24$state)

## Container vectors
polls.margin24 <- result.margin24 <- trumpvotes_pred <- trumpvotes_act <- 
  rep(NA, length(states))

names(polls.margin24) <- names(result.margin24) <- 
  names(trumpvotes_pred) <- names(trumpvotes_act)  <-as.character(unique(states))
```


```{r}
## Loop
for(i in 1:length(states)){
  subdata <- subset(polls24, state == states[i] )
  subdata <- subset(subdata, daystoelection < 15 | 
                      daystoelection == min(subdata$daystoelection) )
  result.margin24[i] <- mean(subdata$resultmargin24)
  polls.margin24[i] <- mean(subdata$pollmargin24)
  trumpvotes_pred[i] <- ifelse(mean(subdata$pollmargin24) > 0, 
                            unique(subdata$ev_total), 0)
  trumpvotes_act[i] <- ifelse(mean(subdata$resultmargin24) > 0, 
                            unique(subdata$ev_total), 0)
 }
sum(trumpvotes_pred) # predicted
sum(trumpvotes_act) # predicted

```

***Less competitive states***

We also need to assign Trump votes to states not included in the poll data. Here, we will continue to predict based on the results of the 2020 election.

```{r}
## Subset only states without polling data for Harris
elecresults_noncomp <- subset(results2024, (! state %in% states))

## predicted EV
trumpvotes_lesscomp_pred <- ifelse(elecresults_noncomp$called2020 == "R", elecresults_noncomp$ev_total, 0) 
names(trumpvotes_lesscomp_pred) <- elecresults_noncomp$state

## actual EV
trumpvotes_lesscomp_result <- ifelse(elecresults_noncomp$called2024 == "R", elecresults_noncomp$ev_total, 0) 
names(trumpvotes_lesscomp_result) <- elecresults_noncomp$state



# predicted Trump votes from less competitive states vs. actual
sum(trumpvotes_lesscomp_pred) 
sum(trumpvotes_lesscomp_result) 
```

The overall prediction combines the less competitive and competitive states:

```{r}
predictedEV <- c(trumpvotes_lesscomp_pred,trumpvotes_pred)
sum(predictedEV)
actualEV <- c(trumpvotes_lesscomp_result, trumpvotes_act)
sum(actualEV)
```

### Check Accuracy

#### Quantitative Measures of Accuracy

From the polls, let's calculate two common measures of prediction error: bias (the average prediction error) and root-mean-squared error (a typical magnitude of the prediction error).

```{r}
## Calculate Bias (Predicted Trump - True Trump)
predictionerror <- polls.margin24 -result.margin24 
bias <- mean(predictionerror)
bias

## Root Mean Squared Error
sqrt(mean((predictionerror)^2))
```

```{r}
## Histogram of Prediction Errors to Show Bias
hist(predictionerror, 
     xlab = "Prediction Error (Predicted Trump Margin - Actual)",
     main = "Histogram of Prediction Error in Latest Polls")
abline(v=mean(predictionerror), col="red",lwd=2)
abline(v=0, col="blue", lwd=2)
```

What do these results suggest about the polls?


#### Classification

Instead of quantifying how far we were off, let's see where we were right vs. where we were wrong.

Classification

-   true positive: correctly predicting Trump to be the winner
-   false positive: incorrectly predicting Trump to be the winner
-   true negative: correctly predicting Trump to be the loser
-   false negative: incorrectly predicting Trump to be the loser

Confusion Matrix

Let's classify our predictions.

```{r}
actualwins <- ifelse(trumpvotes_act > 0, "Trump Won", "Harris Won")
predictedwins <- ifelse(trumpvotes_pred > 0, "Trump Won", "Harris Won")
```

```{r}
table(predictedwins, actualwins)
```

Where did the polls get it wrong?

```{r}
actualwins[actualwins != predictedwins]
trumpvotes_act[actualwins != predictedwins]
```

What's your conclusion?

-   Are the polls alright?
-   How could you improve the prediction?
-   Wait a second... why even poll?


Nate Silver's analysis of the 2025 polls is available here:https://www.natesilver.net/p/so-how-did-the-polls-do-in-2024-its 

![](images/silver24pollerror.png)

