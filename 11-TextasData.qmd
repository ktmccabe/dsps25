# Text as Data {#text}

Recall that we said, four primary goals of social science include:

-   **Describe** and measure
    -   Has the U.S. population increased?
-   **Explain**, evaluate, and recommend (study of causation)
    -   Does expanding Medicaid improve health outcomes?
-   **Predict**
    -   Who will win the next election?
-   **Discover**
    -   How do policies diffuse across states?

In this section, we start to explore the goal of discovery, seeing what we can learn from text as data.

## Why text?

Words (can) matter. Patterns of word usage can be suggestive of deeper divides.

![](images/deadspin.png){width="80%"}

Article from [Deadspin](https://deadspin.com/which-words-are-used-to-describe-white-and-black-nfl-pr-1573683214)

![](images/wordsmass.png)

Article from [NY Times](https://www.nytimes.com/interactive/2016/06/13/us/politics/politicians-respond-to-orlando-nightclub-attack.html)

***Why Use R to analyze text?***

-   Assist in reading large amounts of text

![](images/greg1.jpg){width="40%"} ![](images/greg2.jpg){width="40%"}

-   Efficiently summarize text through quantifying text attributes
-   (Can) remove some subjectivity in coding text, allow to discover aspects of text unknown a priori

## R Packages for text

Packages are like apps on your phone. They give you additional functionality. To use the tools in a package you first have to install it.

```{r, eval=F}
install.packages("sotu", dependencies = TRUE)
install.packages("dplyr", dependencies = TRUE)
install.packages("tidytext", dependencies = TRUE)
install.packages("quanteda", dependencies = TRUE)
install.packages("wordcloud", dependencies = TRUE)
install.packages("SnowballC", dependencies = TRUE)
```

After you install it, just like on a phone, anytime you want to use the app, you need to open it. In R, we do that with `library()`.

```{r, message=F, warning=F}
library(sotu)
library(dplyr)
library(tidytext)
library(quanteda)
library(wordcloud)
library(SnowballC)
```

## Application: State of the Union

The `sotu` package includes a dataset with the text of every U.S. State of the Union speech. It also includes second dataset with information about the speech. When datasets are stored in a package, you can add them to your environment through the `data()` function.

```{r}
data(sotu_meta)
data(sotu_text)
```

We are going to "bind" these together into a new dataframe. That way, the `sotu_text` is a variable inside of our `speeches` dataframe.

```{r}
# Combine metadata and text 
speeches <- cbind(sotu_meta, text=sotu_text)
speeches$doc_id <- speeches$X # make an id for each row

# SOTU speeches
range(speeches$year)
```

### Cleaning Text

Note that when working with raw text data, we usually do want our variables to be character variables and not factor variables. Here, every cell is not a category. Instead, it is a speech!

```{r}
class(speeches$text)
```

Text is messy data. We may want to spruce it up a bit by removing some of the non-essential characters and words, and moving everything to lowercase.

```{r}
## Example of speech
speeches$text[1]
```

```{r}
# break up the text by word
tokens <- unnest_tokens(tbl = speeches,
                output = word, 
                input = text, 
                token = "words", # one-token-per-row format
                to_lower = TRUE, # lowercase
                strip_numeric = TRUE, # strip numbers
                strip_punct = TRUE) # strip punctuation
head(tokens)

## further clean the text
tokens <-  subset(tokens, !word %in% stop_words$word)  # remove stopwords
head(tokens)

## could also stem the words
#tokens$word <- wordStem(tokens$word, language = "en")

```

Note: What you might consider non-essential could differ depending on your application. Maybe you want to keep numbers in your text, for example.


### Word Frequency

We count the number of times each word appears in each speech and sort them in descending order.

```{r}
## introducing some tidyverse tools %>%
token_counts <- tokens %>%
  count(doc_id, word, sort = TRUE)
head(token_counts)
```

Here are the most frequent words for the first and last speeches.

```{r}
# Word frequencies for first and last speech
head(subset(token_counts, doc_id == 1), 10)

head(subset(token_counts, doc_id == 240), 10)
```

Note: these are somewhat generic words.

### Wordcloud
We can create a wordcloud of these speeches. Note that you can add color to the wordcloud using the same `col` argument. You can also assign a vector of colors and indicate `ordered.colors=T` to make sure they show up according to the order of the colors you assign.

Sometimes you will get a warning message that not all words could fit. You can increase the size of your plotting window in RStudio or adjust the scale() argument, which is similar to our cex arguments in past plotting functions.  Scale reflects the range of the size of the words.

```{r}
first_speech <- subset(token_counts, doc_id == 1)
first_speech$colors <- ifelse(first_speech$word=="citizens", "red2", "black")
wordcloud(first_speech$word, first_speech$n, max.words = 20,
          col=first_speech$colors,
          ordered.colors = T,
          scale=c(3.5, 0.25))

```

## Word Importance

We use tf-idf (term frequency - inverse document frequency) as a way to pull out uniquely important/relevant words for a given character.

-   Relative frequency of a term inversely weighted by the number of documents in which the term appears.
-   Functionally, if everyone uses the word "know," then it's not very important for distinguishing characters/documents from each other.
-   We want words that a speech used frequently, that other speeches use less frequently

```{r}
tfidf <- token_counts %>%
  bind_tf_idf(word, doc_id, n) %>%
  arrange(desc(tf_idf))

# Most unique words in first and last speech
head(subset(tfidf, doc_id == 1), 10)
head(subset(tfidf, doc_id == 240), 10)


## wordcloud
first_speech_tfidf <- subset(tfidf, doc_id == 1)
wordcloud(first_speech_tfidf$word, first_speech_tfidf$tf_idf, max.words = 30,
          scale=c(2.5, 0.25))

```


Instead of a wordcloud, we could also present results as barplots.

```{r}
barplot(first_speech_tfidf$tf_idf[1:10], 
        cex.axis=.6,
        names=first_speech_tfidf$word[1:10],
        cex.names=.5,
        main= "Most `Important' 1790 SOTU Words (tf-idf)", 
        horiz = T, # flips the plot
        las=2,
        xlim=c(0, .015))

```

## Document length

Are the length of speeches changing? The `nchar()` function tells you the number of characters in a "string."

```{r}
speeches$speechlength <- nchar(speeches$text)
```

Let's plot the length of speeches over time and annotate with informative colors and labels.

Is the length of speeches changing?

```{r}
plot(x=1:length(speeches$speechlength), y= speeches$speechlength, 
    pch=15,
     xaxt="n",
     xlab="", 
     ylab = "Number of Characters")

## add x axis
axis(1, 1:length(speeches$speechlength), labels=speeches$year, las=3, cex.axis=.7)
```

We can add color to distinguish written vs. spoken speeches

```{r}
speechcolor <- ifelse(speeches$sotu_type == "written", "black", "green3")
plot(x=1:length(speeches$speechlength), y= speeches$speechlength, 
     xaxt="n", pch=15,
     xlab="", 
     ylab = "Number of Characters",
     col = speechcolor)

## add x axis
axis(1, 1:length(speeches$speechlength), labels=speeches$year, las=3, cex.axis=.7)

## add legend
legend("topleft", c("spoken", "written"), 
       pch=15, 
       col=c("green3", "black"), bty="n")
```

### Counting tokens instead of characters

```{r}
token_totals <- tapply(token_counts$n, token_counts$doc_id, sum)

# Convert to a data frame
token_totals <- data.frame(
  doc_id = as.integer(names(token_totals)),
  token_count = as.integer(token_totals)
)

speeches <- merge(speeches, token_totals, by = "doc_id", all.x = TRUE)

```


```{r}
speechcolor <- ifelse(speeches$sotu_type == "written", "black", "green3")
plot(x=1:length(speeches$token_count), y= speeches$token_count, 
     xaxt="n", pch=15,
     xlab="", 
     ylab = "Number of Words",
     col = speechcolor)

## add x axis
axis(1, 1:length(speeches$token_count), labels=speeches$year, las=3, cex.axis=.7)

## add legend
legend("topleft", c("spoken", "written"), 
       pch=15, 
       col=c("green3", "black"), bty="n")

```
### Hypothesis Tests

Can we test whether written speeches are significantly different in length from spoken speeches?

Null hypothesis: No difference in speech length

Can we reject the null?
```{r}
t.speechlength <- t.test(speechlength ~ sotu_type, data=speeches)
t.speechlength

round(t.speechlength$p.value, digits=4)
```

What about a different hypothesis: Have speeches become shorter over time? For every one year increase, do we see an average decline in speech length?

Null hypothesis: No relationship, b=0

Can we reject the null? No!

```{r}
fit <- lm(speechlength ~ year, data=speeches)
summary(fit)
```

Note that you can add multiple predictors to "control" on one variable while studying the relationship with others.

```{r}
fit2 <- lm(speechlength ~ year + sotu_type, data=speeches)
summary(fit2)
```


Now that we control on speech type, we see that time is associated with a slight increase in length.


## Dictionary Analysis

We can characterize the content of speeches in different ways. For example, we can see if speeches mention specific words, such as "terrorism."


```{r}
# Define a set of related terms
terror_terms <- c("terror", "terrorism", "terrorist", 
                  "counterterror")

# Subset to just those tokens
terror_tokens <- subset(token_counts, word %in% terror_terms)
```

Below we calculate sums. The first uses functions we have already covered in the course: `tapply` and `data.frame`.

```{r}
# Count terrorism-related tokens per doc_id using tapply
terror_counts <- tapply(terror_tokens$n, terror_tokens$doc_id, sum)

head(terror_counts)
```

We now create a data.frame. We did this recently in our prediction module. This just reformats our tapply output.

```{r}
# Convert named vector to data frame
terror_counts <- data.frame(
  doc_id = names(terror_counts),
  terror_word_count = as.integer(terror_counts)
)
head(terror_counts)
```


We can now merge this back in with our speeches data so that the information is attached to our original dataset.

```{r}
# Merge with speeches
speeches <- merge(speeches, terror_counts, by = "doc_id", all.x = TRUE)
```

Lastly, we have just a little cleanup. We want all of the speeches to have a number, so we have to replace any missing values with 0, indicating the absence of any "terrorism" word.

```{r}
# Replace NA with 0 (docs that had none of those terms)
speeches$terror_word_count[is.na(speeches$terror_word_count)] <- 0

```

Let's look at the use of terrorism words by president.
```{r}
sort(tapply(speeches$terror_word_count, speeches$president, sum), 
     decreasing=T)[1:10]
```

What are possible limitations of this analysis?

# In-Class Exercise: Choose your Own Adventure

## Choose your own adventure
In this health check, pick and load the text as dataset of your choice to work with. Options include:

  - `constitutions.csv`: The data set contains the raw textual information about the preambles of constitutions around the world. We focus on the preamble of each constitution, which typically states the guiding purpose and principles of the rest of the constitution.
    - `country`: The country name 
    - `year`: The year the constitution was created 
    - `preamble`: Raw text of the constitution's preamble 
  - Beyonce lyrics: The dataset contains lyrics to many Beyonce songs at the "line level" (though was created in 2020, so is missing a few) 
    - `line`: Contains the line of text
    - `song_name`: Contains the name of the song
  - Taylor Swfit lyrics: The dataset contains the lyrics to many Taylor Swift songs.
    - `lyric`: Contains the line of text
    - `track_name`: Contains the name of the track
    - `album_name`: Containes the name of the album
  - Stranger Things dialogue: The data set contains dialogue from the show Stranger Things (dataset from 2023)
    - `raw_text`: Contains the line of dialogue
    - `season_episode`: Indicates which season and episode the dialogue comes from
    - `episdode`, `season`: Separately indicates the episode number or season number


Let's read in the data, load required packages. Verify that the text variable in your chosen dataset is a "character" variable and that you can see the text output of the first observation.

First, load some text packages

```{r, warning=F, message=F}
library(dplyr)
library(tidytext)
library(quanteda)
library(wordcloud)
library(SnowballC)
```

```{r, message=F}
## constitution
constitution <- read.csv("https://raw.githubusercontent.com/ktmccabe/teachingdata/main/constitution.csv")
constitution$doc_id <- 1:nrow(constitution) #make unique doc_id variable
class(constitution$preamble) # text variable
constitution$preamble[1]

```


```{r}
## beyonce
beyonce <-  read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-09-29/beyonce_lyrics.csv')
beyonce$doc_id <- 1:nrow(beyonce) #make unique doc_id variable
class(beyonce$line)
beyonce$line[1]
```


```{r}
taylor <- read.csv("https://raw.githubusercontent.com/ktmccabe/teachingdata/refs/heads/main/taylor.csv")
taylor$doc_id <- 1:nrow(taylor) #make unique doc_id variable
class(taylor$lyric)
taylor$lyric[1]
```


```{r}
## stranger things
strangerthings <- read.csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2022/2022-10-18/stranger_things_all_dialogue.csv')
strangerthings$season_episode <- paste(strangerthings$season, 
                                       strangerthings$episode, sep = "_")
strangerthings$doc_id <- 1:nrow(strangerthings) #make unique doc_id variable
class(strangerthings$raw_text)
strangerthings$raw_text[1]
```


### Problem 1

Let's pre-process our texts to make them easier to analyze. Unnest the text by the unit of "words." Remove punctuation and make lowercase. You can decide whether you want to remove numbers based on your use case. Store the results in an object called `tokens`. This is an arbitrary name but will help us all work together. You should use the `unnest` code from our course notes, but you likely have a different `input` column.

Report the 6 first words in the data by using `head(tokens$word)` after you unnest.


```{r}
# break up the text by word

```

After creating the tokens object, further remove stop words by running the code here: `tokens <-  subset(tokens, !word %in% stop_words$word)`  


```{r}
## further clean the text
```

### Problem 2

Word frequency. Count up the number of times each word is used in a given unit analysis of interest. This could be by the `doc_id` variable or some higher level of aggregation such as `track_name`,`song_name` or `season_episode`. Store these counts in an object called `token_counts`. And report `head(token_counts)`

```{r}
## use the count function

```


### Problem 3

Wordclouds! Pick a particular subset of the `token_counts` to analyze. For example, you might pick a particular `doc_id`, `track_name`, etc.

```{r}
# subset the data and then use wordcloud()

```



### Problem 4

Pick a topic to explore in your text. Create a vector of words that represent this topic. Sum up the number of times these words are used and merge them back into your dataset.

```{r}
# create the vector of words

# subset the token counts data using subset() to only rows that have those words

# use tapply to sum up the words

# format as a dataframe


# merge back into your original dataset

```



### Extra if time

Conduct a `t.test()` to compare average mentions of the topic you chose across two different groups in your data. E.g., this could be across two different seasons of Stranger Things, different year ranges on constitutions data, different albums of songs, etc. 

```{r}
# t.test code here

```

### Example application

<details>

<summary>Try on your own, then expand for an example.</summary>

```{r}
taylor <- read.csv("https://raw.githubusercontent.com/ktmccabe/teachingdata/refs/heads/main/taylor.csv")
taylor$doc_id <- 1:nrow(taylor) #make unique doc_id variable
class(taylor$lyric)
taylor$lyric[1]
```

```{r}
# break up the text by word
tokens_taylor <- unnest_tokens(tbl = taylor,
                output = word, 
                input = lyric, 
                token = "words", # one-token-per-row format
                to_lower = TRUE, # lowercase
                strip_numeric = TRUE, # strip numbers
                strip_punct = TRUE) # strip punctuation
head(tokens_taylor$word)

tokens_taylor <-  subset(tokens_taylor, !word %in% stop_words$word)  

token_counts_taylor <- tokens_taylor %>%
  count(track_name, word, sort = TRUE)
head(token_counts_taylor)


red_words <- subset(token_counts_taylor, track_name == "Red (Taylor's Version)")
colors_taylor <- ifelse(red_words$word=="red", "red3", "black")
wordcloud(red_words$word, red_words$n, max.words = 20,
          col=colors_taylor,
          ordered.colors=T,
          scale=c(2.5, .25))


# create the vector of words
love <- c("love", "loving", "marry")

# subset the token counts data using subset() to only rows that have those words
love_taylor <- subset(token_counts_taylor, word %in% love)

# use tapply to sum up the words
love_taylor_counts <- tapply(love_taylor$n, love_taylor$track_name, sum)

# format as a dataframe
love_taylor_counts <- data.frame(
  track_name = names(love_taylor_counts),
  love_word_count = as.integer(love_taylor_counts)
)

# merge back into your original dataset
taylor <- merge(taylor, love_taylor_counts, by = "track_name", all.x = TRUE)
taylor$love_word_count[is.na(taylor$love_word_count)] <- 0


# t.test code here
taylor_track <- taylor %>% select(love_word_count, track_name, album_name)
taylor_track <- unique(taylor_track)

taylor_track <- subset(taylor_track, album_name %in% c("reputation", "1989 (Taylor's Version)"))
t.test(love_word_count ~ album_name, data=taylor_track)


```

</details>

# Application Programming Interfaces

Application programming interfaces (APIs) are tools that allow you to search a large database to extract specific types of information. Social scientists often work with APIs to extract data from social media platforms, government agencies (e.g., U.S. Census), and news sites, among others.

Organizations that develop these APIs can control what types of information researchers can access. Often, they set limits on the types and quantities of information someone can collect. Companies also often monitor who accesses the information by requiring people to sign up for access, apply for access, and/or pay for access.

## Example: Google Trends

Oftentimes, developers that use R will create "wrappers" for using APIs. These are R packages that have R-friendly functions for asking the API to extract information.

An example of one of these is the `gtrendsR` package which queries Google to understand the popularity of different topics using key word search terms.

We can install the R packages below. Remember, you only have to do the install line once.

We are going to install the development version of the package, which includes the most recent updates the package authors have made, which often happen prior to when they officially provide them to the R respository. To do this, we need the package `devtools`. Occasionally, this can create trouble for some computers. If you have trouble installing `devtools`, you can focus on other examples instead. This is not essential for the completion of the course.

```{r, eval=F}
install.packages("devtools")
install.packages("curl")
```

```{r, eval=F}
devtools::install_github("PMassicotte/gtrendsR")
```

Each time we use it, we need to use `library()`

```{r}
library(gtrendsR)
```

Now we have access. Note that most functions that let you extract information from websites, social media, etc., have what are called "rate limits" that limit the size and quantity of your calls for information. If you get an error when making a query, this might be what is happening. 

### Popularity of Side Dishes

To demonstrate how this works, we can ask for data on the popularity of "green bean casserole" and "mashed potatoes" in NJ.

The first line asks and stores this information

```{r}
res <- gtrends(c("green bean casserole",
                 "mashed potatoes"), geo = c("US-NJ"))
```

### Saving R Objects

After you extract data from online, you may want to save them as a hard data file on your computer. This way if you close RStudio, you can reproduce the data.

R allows you to save any R object as an .RData file that can be opened with the `load()` command. This is discussed on pg. 24 of QSS [Chapter 1](https://assets.press.princeton.edu/chapters/s11025.pdf).

We can demonstrate this now by saving `res` as an RData object. It will automatically save to your working directory, but you can also add a subfolder or alternative file path.

```{r, eval=F}
save(res, file = "sideresults.RData")
```

Then, you can load the file (if you happen to close R/RStudio, restart your computer, etc.) with the load command.

```{r}
load("sideresults.RData")
```

We can take a look at the data we extracted. Trend results are re-scaled based on relative popularity, not absolute search volume.
```{r}
head(res$interest_over_time)
```

And this line plots the results. The package has a built-in version of the R `plot` function.

```{r}
plot(res)
```



## Perspective API

Another API that can be used with text as data is Google's Perspective API. It uses statistical machine learning models based trained on real-world comments to score text on the likelihood it will be perceived as toxic. More information here: https://www.perspectiveapi.com/.

This, like many APIs, requires a "key" and "secret" to use. Think of these like passwords.We are going to use an R package that serves as a "wrapper" for the API, making it easier to access the API with R-friendly functions. The github for the package describes the steps you can personally take to set up a perspective API account. We will just use my account in class. https://github.com/favstats/peRspective?tab=readme-ov-file

We will use the development version of the package.

```{r, eval=F}
devtools::install_github("favstats/peRspective")
```

We open the package below.

```{r}
library(peRspective)
```

From the package github documentation: "peRspective functions will read the API key from environment variable `perspective_api_key`. In order to add your key to your environment file, you can use the function `edit_r_environ()` from the usethis package. If your function does not work, you likely need to install `usethis` as a package first."

```{r, eval=F}
usethis::edit_r_environ()

```

"This will open your .Renviron file in your text editor. Now, you can add the following line to it:"

```{r, eval=F}
perspective_api_key="YOUR_API_KEY"
```

Now you should be ready to use the main function, `prsp_score` which will score text based on the chance it would be perceived as toxic, among other dimensions of text sentiment/content. 

For example, let's pull two recent tweets from Vice President JD Vance to demonstrate variation in the scores:

```{r}
jd1 <- "I have an extraordinary tolerance for disagreements and criticisms from the various people in our coalition. But I am a very loyal person, and I have zero tolerance for scumbags attacking my staff. And yes, *everyone* who I've seen attack Buckley with lies is a scumbag."
text_scores1 <- prsp_score(
           text = jd1, 
           languages = "en",
           score_model = peRspective::prsp_models
           )
text_scores1$TOXICITY
```

```{r}
jd2 <- "Today's jobs report shows that the Trump economic plan is working: 119,000 new jobs when economists thought we would only add 52,000.Wages are continuing to outpace inflation and manufacturing hours are increasing. We've got a lot more work to do, but these are great signs."
text_scores2 <- prsp_score(
           text = jd2, 
           languages = "en",
           score_model = peRspective::prsp_models)
text_scores2$TOXICITY
```

If we wanted to score several comments from a dataset, we could create a loop to repeat the function for each comment. Here is a short example creating our tweets as a dataframe first, and then storing toxicity scores in that dataset as a new column.

```{r, eval=F}
jddata <- data.frame(text=c(jd1, jd2), toxicity=NA)

for(i in 1:length(jddata$text)){
  text_scores <- prsp_score(
           text = jddata$text[i], 
           languages = "en",
           score_model = peRspective::prsp_models)
  jddata$toxicity[i] <- text_scores$TOXICITY
}
jddata
```

We can then save the output so we don't have to re-score the data every time we use R.

```{r, eval=F}
save(jddata, file="jddata.RData")
```

```{r}
load("jddata.RData")
jddata
```

## Example: Census API 

As another example of an API, the U.S. Census has an API that allows researchers to extract nicely formatted data summaries of different geographic units (e.g., all zip codes in the U.S.).

-   Researchers can sign up [here](https://api.census.gov/data/key_signup.html) for an API "key" which allows the organization to monitor who is accessing what information.

Researchers Kyle Walker and Matt Herman have made an R package that makes working with the API easier.

-   Example: `tidycensus` found [here](https://walker-data.com/tidycensus/articles/basic-usage.html) allows you to search Census data by providing the variables you want to extract

![](images/tidycensus.png){width="45%"}


APIs can make a social scientist's life easier by providing an efficient way to collect data. Without an API, researchers might have to resort to manually extracting information from online or writing an ad hoc set of code to "scrape" the information off of websites. This can be time consuming, against an organization or company's policy, or even impossible in some cases. APIs are powerful and efficient.

However, because researchers cannot control the API, the downside is at any given time, an organization could change or remove API access. Researchers might also not have the full details of what information is included in the API, potentially leading to biased conclusions from the data. APIs are great, but we should use them with caution.

### Using the tidycensus

```{r, eval=FALSE}
install.packages("tidycensus", dependencies=T)
```

Load the package
```{r}
library(tidycensus)
library(tidyverse)
```

Add your personal API KEY. Treat this like a password, and do not share
```{r, eval=FALSE}
census_api_key("YOUR API KEY GOES HERE")
```
Search for the variables you are interested in by loading a dataframe relevant to your research question.

```{r, eval=F}
## Load variables associated with Census/ACS/ACS-5
v23 <- load_variables(2023, "acs5", cache = TRUE)
```

Extract data for the variable. We will extract the median income for NJ counties.

```{r, eval=F}
## Getting median income across NJ counties
nj <- get_acs(geography = "county", 
              variables = c(medincome = "B19013_001"), 
              state = "NJ", 
              year = 2023)
```


```{r, eval=FALSE}
nj
```

We can save the data to our computer.
```{r, eval=FALSE}
save(nj, file="njmedincome.RData")
```

What can we do with this dataset? Analyze it or merge it with other data to anaalyze!

# Preview of mapping in R

For example, to preview our next section, we can map the median income across counties in NJ.

```{r}
## Load income data
load("njmedincome.RData")

## Load mapping package in R
library(maps)

## create a map of NJ
map(database="county", regions = "New Jersey")
```

Work to merge mapping and income data together (any outside data)

```{r}
## extract mapping data
njcounties <- map_data("county", region="New Jersey")

## clean income data to match mapping data
nj$NAME <- gsub(" County, New Jersey", "", nj$NAME)
nj$NAME <- tolower(nj$NAME)

## merge the data
njcounties <- merge(njcounties, nj, 
                    by.x="subregion", by.y = "NAME", 
                    all.x=TRUE, all.y=F)
```

Use ggplot to help map the data

```{r}
library(ggplot2)
ggplot()+
  
  ## create an nj county-level plot
  geom_polygon(data=njcounties, aes(x=long, y=lat, 
                                    group=group, 
                                    fill=estimate),
               colour="white")+
  ## Shade the map according to the vote share
  scale_fill_gradient(name="Median Income %", low="yellow", high="red")+
  
  ## remove background
  theme_void()+
  
  ggtitle("2023 ACS-5 Year Median Income")+
  coord_quickmap()
```

